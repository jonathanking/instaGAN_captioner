{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26769b0ecbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/build/anaconda3/envs/pytorch_src2/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "for i in range(1):\n",
    "    img = Image.open('/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg')\n",
    "    img = img.resize((256,256))\n",
    "\n",
    "e = time.time()\n",
    "print(np.asarray(img).transpose(2,0,1).shape)\n",
    "\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4779  JPGs in lifeandthyme.\n",
      "lifeandthyme - finished processing - output shape - torch.Size([4779, 3, 256, 256])\n",
      "388  JPGs in kimberleyhasselbrink.\n",
      "kimberleyhasselbrink - finished processing - output shape - torch.Size([388, 3, 256, 256])\n",
      "1500  JPGs in julieskitchen.\n",
      "julieskitchen - finished processing - output shape - torch.Size([1500, 3, 256, 256])\n",
      "570  JPGs in hungrybetches.\n",
      "hungrybetches - finished processing - output shape - torch.Size([570, 3, 256, 256])\n",
      "1853  JPGs in food_feels.\n",
      "food_feels - finished processing - output shape - torch.Size([1853, 3, 256, 256])\n",
      "1  JPGs in local_milk.\n",
      "local_milk - finished processing - output shape - torch.Size([1, 3, 256, 256])\n",
      "946  JPGs in izyhossack.\n",
      "izyhossack - finished processing - output shape - torch.Size([946, 3, 256, 256])\n",
      "1409  JPGs in candidsbyjo.\n",
      "candidsbyjo - finished processing - output shape - torch.Size([1409, 3, 256, 256])\n",
      "304  JPGs in girleatworld.\n",
      "girleatworld - finished processing - output shape - torch.Size([304, 3, 256, 256])\n",
      "318  JPGs in silverspies.\n",
      "silverspies - finished processing - output shape - torch.Size([318, 3, 256, 256])\n",
      "11739  JPGs in foodiemagician.\n",
      "foodiemagician - finished processing - output shape - torch.Size([11739, 3, 256, 256])\n",
      "1356  JPGs in wrightkitchen.\n",
      "wrightkitchen - finished processing - output shape - torch.Size([1356, 3, 256, 256])\n",
      "13  JPGs in thecuriouspear.\n",
      "thecuriouspear - finished processing - output shape - torch.Size([13, 3, 256, 256])\n",
      "1976  JPGs in pinchofyum.\n",
      "pinchofyum - finished processing - output shape - torch.Size([1976, 3, 256, 256])\n",
      "3134  JPGs in fitmencook.\n",
      "fitmencook - finished processing - output shape - torch.Size([3134, 3, 256, 256])\n",
      "819  JPGs in fooodeelicious.\n",
      "fooodeelicious - finished processing - output shape - torch.Size([819, 3, 256, 256])\n",
      "1561  JPGs in damn_delicious.\n",
      "damn_delicious - finished processing - output shape - torch.Size([1561, 3, 256, 256])\n",
      "1  JPGs in modernhoney.\n",
      "modernhoney - finished processing - output shape - torch.Size([1, 3, 256, 256])\n",
      "1559  JPGs in ladyandpups.\n",
      "ladyandpups - finished processing - output shape - torch.Size([1559, 3, 256, 256])\n",
      "4113  JPGs in cannellevanille.\n",
      "cannellevanille - finished processing - output shape - torch.Size([4113, 3, 256, 256])\n",
      "2521  JPGs in xlbcr.\n",
      "xlbcr - finished processing - output shape - torch.Size([2521, 3, 256, 256])\n",
      "13  JPGs in dametravelerfoodie.\n",
      "dametravelerfoodie - finished processing - output shape - torch.Size([13, 3, 256, 256])\n",
      "657  JPGs in csaffitz.\n",
      "csaffitz - finished processing - output shape - torch.Size([657, 3, 256, 256])\n",
      "2108  JPGs in anisa.sabet.\n",
      "anisa.sabet - finished processing - output shape - torch.Size([2108, 3, 256, 256])\n",
      "601  JPGs in culinarybrodown.\n",
      "culinarybrodown - finished processing - output shape - torch.Size([601, 3, 256, 256])\n",
      "105  JPGs in marthastewart.\n",
      "marthastewart - finished processing - output shape - torch.Size([105, 3, 256, 256])\n",
      "440  JPGs in shivesh17.\n",
      "shivesh17 - finished processing - output shape - torch.Size([440, 3, 256, 256])\n",
      "3052  JPGs in rachlmansfield.\n",
      "rachlmansfield - finished processing - output shape - torch.Size([3052, 3, 256, 256])\n",
      "319  JPGs in danella_chalmers.\n",
      "danella_chalmers - finished processing - output shape - torch.Size([319, 3, 256, 256])\n",
      "8245  JPGs in thefeedfeed.\n",
      "thefeedfeed - finished processing - output shape - torch.Size([8245, 3, 256, 256])\n",
      "2  JPGs in alison_wu.\n",
      "alison_wu - finished processing - output shape - torch.Size([2, 3, 256, 256])\n",
      "241  JPGs in igbrunchclub.\n",
      "igbrunchclub - finished processing - output shape - torch.Size([241, 3, 256, 256])\n",
      "666  JPGs in becausegb.\n",
      "becausegb - finished processing - output shape - torch.Size([666, 3, 256, 256])\n",
      "540  JPGs in alisoneroman.\n",
      "alisoneroman - finished processing - output shape - torch.Size([540, 3, 256, 256])\n",
      "3583  JPGs in new_fork_city.\n",
      "new_fork_city - finished processing - output shape - torch.Size([3583, 3, 256, 256])\n",
      "2301  JPGs in lindsaymaitland.\n",
      "lindsaymaitland - finished processing - output shape - torch.Size([2301, 3, 256, 256])\n",
      "1051  JPGs in gatherandfeast.\n",
      "gatherandfeast - finished processing - output shape - torch.Size([1051, 3, 256, 256])\n",
      "2054  JPGs in rushyama.\n",
      "rushyama - finished processing - output shape - torch.Size([2054, 3, 256, 256])\n",
      "7928  JPGs in foodbabyny.\n",
      "foodbabyny - finished processing - output shape - torch.Size([7928, 3, 256, 256])\n",
      "2460  JPGs in joythebaker.\n",
      "joythebaker - finished processing - output shape - torch.Size([2460, 3, 256, 256])\n",
      "4  JPGs in sarahlynnfitness.\n",
      "sarahlynnfitness - finished processing - output shape - torch.Size([4, 3, 256, 256])\n",
      "427  JPGs in discover.vi.\n",
      "discover.vi - finished processing - output shape - torch.Size([427, 3, 256, 256])\n",
      "573  JPGs in organicandhappy.\n",
      "organicandhappy - finished processing - output shape - torch.Size([573, 3, 256, 256])\n",
      "2367  JPGs in halfbakedharvest.\n",
      "halfbakedharvest - 1600 images processed\r"
     ]
    }
   ],
   "source": [
    "import os,sys,cv2,gc,json,lzma,re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "xdim = 256\n",
    "ydim = 256\n",
    "metadata = []\n",
    "\n",
    "dfolder = '/home/jok120/ml/proj/data/old_data/tags/tmp/'\n",
    "\n",
    "for folder in os.listdir(dfolder):\n",
    "    n = 0\n",
    "    n_int = 1\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    path_folder = os.path.join(dfolder,folder)\n",
    "\n",
    "    if not (os.path.isdir(path_folder)):# and folder.startswith('#')):\n",
    "        continue\n",
    "        \n",
    "    jpgCounter = len(glob.glob1(path_folder,\"*.jpg\"))\n",
    "    print(jpgCounter, \" JPGs in {}.\".format(folder))\n",
    "    images = torch.empty(jpgCounter,3,xdim,ydim)\n",
    "\n",
    "    for f in os.listdir(path_folder):\n",
    "        fname, fext = os.path.splitext(f)\n",
    "\n",
    "        if fext != '.jpg':\n",
    "            continue\n",
    "\n",
    "        #img = imread(os.path.join(dfolder,folder,f)) / 255\n",
    "        #img = torch.from_numpy(cv2.resize(img, dsize=(ydim,xdim)).transpose(2,0,1)[None,:]).type(torch.FloatTensor)\n",
    "        img = Image.open(os.path.join(dfolder,folder,f))\n",
    "        img = img.resize((ydim,xdim))\n",
    "        img = np.asarray(img) / 255\n",
    "        img = torch.from_numpy(img.transpose(2,0,1)).type(torch.FloatTensor)\n",
    "\n",
    "        json_filename = os.path.join(dfolder,folder,re.sub('UTC_(\\\\d+)','UTC',fname)+'.json.xz')\n",
    "\n",
    "        try:\n",
    "            json_str = lzma.open(json_filename).read()\n",
    "        except:\n",
    "            continue\n",
    "        json_data = json.loads(json_str)\n",
    "        metadata.append(json_data)\n",
    "        \n",
    "        images[n] = img\n",
    "        n += 1\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print('{} - {} images processed'.format(folder,n),end='\\r')\n",
    "\n",
    "    print('{} - finished processing - output shape - {}'.format(folder,images.shape))\n",
    "\n",
    "    torch.save(images,os.path.join(dfolder,'{}.tch'.format(folder)))\n",
    "    pkl.dump(metadata,open(os.path.join(dfolder,'{}_metadata.pkl'.format(folder)),'wb'))\n",
    "    \n",
    "    images = torch.empty(0,3,xdim,ydim)\n",
    "    metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "dfolder = '/home/jok120/ml/proj/data/old_data/tags/tmp/'\n",
    "wfolder = '/home/jok120/ml/'\n",
    "\n",
    "X_train = torch.empty((0,3,64,64))\n",
    "X_meta = []\n",
    "\n",
    "N = 1\n",
    "\n",
    "for f in os.listdir(dfolder):\n",
    "    fname, fext = os.path.splitext(f)\n",
    "    \n",
    "    if fext != '.tch':\n",
    "        continue\n",
    "        \n",
    "    X_train = torch.cat([X_train,torch.load(os.path.join(dfolder,f))])\n",
    "    X_meta += pickle.load(open(os.path.join(dfolder,fname+'_metadata.pkl'),'rb'))\n",
    "    print(len(X_train),len(X_meta))\n",
    "    if len(X_train) > 100000:\n",
    "        torch.save(X_train,os.path.join(wfolder,'train_data_{}.tch'.format(N)))\n",
    "        pickle.dump(X_meta,open(os.path.join(wfolder,'train_meta_{}.pkl'.format(N)),'wb'))\n",
    "        \n",
    "        N += 1\n",
    "        \n",
    "        X_train = torch.empty((0,3,64,64))\n",
    "        X_meta = []\n",
    "        \n",
    "torch.save(X_train,os.path.join(wfolder,'train_data_{}.tch'.format(N)))\n",
    "pickle.dump(X_meta,open(os.path.join(wfolder,'train_meta_{}.pkl'.format(N)),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.empty((0,3,64,64))\n",
    "torch.cat([torch.ones(0,3,64,64),torch.ones(8,3,64,64)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(images,os.path.join(dfolder,'data/#homecooking.tch'.format(folder)))\n",
    "pkl.dump(metadata,open(os.path.join(dfolder,'data/{}_metadata.pkl'.format(folder)),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_dlproj)",
   "language": "python",
   "name": "pytorch_dlproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
