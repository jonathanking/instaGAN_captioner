{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26769b0ecbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/build/anaconda3/envs/pytorch_src2/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "for i in range(1):\n",
    "    img = Image.open('/media/kyle/store/insta_data/hashtags/#buzzfeast/2019-02-26_23-00-09_UTC.jpg')\n",
    "    img = img.resize((256,256))\n",
    "\n",
    "e = time.time()\n",
    "print(np.asarray(img).transpose(2,0,1).shape)\n",
    "\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4779 JPGs, 4271 metas in lifeandthyme.\n",
      "lifeandthyme - finished processing - output shape - torch.Size([4778, 3, 256, 256])\n",
      "388 JPGs, 380 metas in kimberleyhasselbrink.\n",
      "kimberleyhasselbrink - finished processing - output shape - torch.Size([387, 3, 256, 256])\n",
      "1500 JPGs, 1494 metas in julieskitchen.\n",
      "julieskitchen - finished processing - output shape - torch.Size([1499, 3, 256, 256])\n",
      "570 JPGs, 563 metas in hungrybetches.\n",
      "hungrybetches - finished processing - output shape - torch.Size([569, 3, 256, 256])\n",
      "1853 JPGs, 1795 metas in food_feels.\n",
      "food_feels - finished processing - output shape - torch.Size([1852, 3, 256, 256])\n",
      "1 JPGs, 1 metas in local_milk.\n",
      "local_milk - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "946 JPGs, 917 metas in izyhossack.\n",
      "izyhossack - finished processing - output shape - torch.Size([945, 3, 256, 256])\n",
      "1409 JPGs, 1387 metas in candidsbyjo.\n",
      "candidsbyjo - finished processing - output shape - torch.Size([1408, 3, 256, 256])\n",
      "304 JPGs, 303 metas in girleatworld.\n",
      "girleatworld - finished processing - output shape - torch.Size([303, 3, 256, 256])\n",
      "318 JPGs, 313 metas in silverspies.\n",
      "silverspies - finished processing - output shape - torch.Size([317, 3, 256, 256])\n",
      "11739 JPGs, 9742 metas in foodiemagician.\n",
      "foodiemagician - finished processing - output shape - torch.Size([11738, 3, 256, 256])\n",
      "1356 JPGs, 1298 metas in wrightkitchen.\n",
      "wrightkitchen - finished processing - output shape - torch.Size([1355, 3, 256, 256])\n",
      "13 JPGs, 13 metas in thecuriouspear.\n",
      "thecuriouspear - finished processing - output shape - torch.Size([12, 3, 256, 256])\n",
      "1976 JPGs, 1938 metas in pinchofyum.\n",
      "pinchofyum - finished processing - output shape - torch.Size([1975, 3, 256, 256])\n",
      "3134 JPGs, 2914 metas in fitmencook.\n",
      "fitmencook - finished processing - output shape - torch.Size([3133, 3, 256, 256])\n",
      "819 JPGs, 744 metas in fooodeelicious.\n",
      "fooodeelicious - finished processing - output shape - torch.Size([818, 3, 256, 256])\n",
      "1561 JPGs, 1538 metas in damn_delicious.\n",
      "damn_delicious - finished processing - output shape - torch.Size([1560, 3, 256, 256])\n",
      "1 JPGs, 1 metas in modernhoney.\n",
      "modernhoney - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "1559 JPGs, 1488 metas in ladyandpups.\n",
      "ladyandpups - finished processing - output shape - torch.Size([1558, 3, 256, 256])\n",
      "4113 JPGs, 4095 metas in cannellevanille.\n",
      "cannellevanille - finished processing - output shape - torch.Size([4112, 3, 256, 256])\n",
      "2521 JPGs, 2329 metas in xlbcr.\n",
      "xlbcr - finished processing - output shape - torch.Size([2519, 3, 256, 256])\n",
      "13 JPGs, 13 metas in dametravelerfoodie.\n",
      "dametravelerfoodie - finished processing - output shape - torch.Size([12, 3, 256, 256])\n",
      "657 JPGs, 644 metas in csaffitz.\n",
      "csaffitz - finished processing - output shape - torch.Size([656, 3, 256, 256])\n",
      "2108 JPGs, 1980 metas in anisa.sabet.\n",
      "anisa.sabet - finished processing - output shape - torch.Size([2107, 3, 256, 256])\n",
      "601 JPGs, 567 metas in culinarybrodown.\n",
      "culinarybrodown - finished processing - output shape - torch.Size([600, 3, 256, 256])\n",
      "105 JPGs, 63 metas in marthastewart.\n",
      "marthastewart - finished processing - output shape - torch.Size([104, 3, 256, 256])\n",
      "440 JPGs, 313 metas in shivesh17.\n",
      "shivesh17 - finished processing - output shape - torch.Size([439, 3, 256, 256])\n",
      "3052 JPGs, 2986 metas in rachlmansfield.\n",
      "rachlmansfield - finished processing - output shape - torch.Size([3051, 3, 256, 256])\n",
      "319 JPGs, 313 metas in danella_chalmers.\n",
      "danella_chalmers - finished processing - output shape - torch.Size([318, 3, 256, 256])\n",
      "8245 JPGs, 8126 metas in thefeedfeed.\n",
      "thefeedfeed - finished processing - output shape - torch.Size([8244, 3, 256, 256])\n",
      "2 JPGs, 2 metas in alison_wu.\n",
      "alison_wu - finished processing - output shape - torch.Size([1, 3, 256, 256])\n",
      "241 JPGs, 240 metas in igbrunchclub.\n",
      "igbrunchclub - finished processing - output shape - torch.Size([240, 3, 256, 256])\n",
      "666 JPGs, 663 metas in becausegb.\n",
      "becausegb - finished processing - output shape - torch.Size([665, 3, 256, 256])\n",
      "540 JPGs, 517 metas in alisoneroman.\n",
      "alisoneroman - finished processing - output shape - torch.Size([539, 3, 256, 256])\n",
      "3583 JPGs, 3543 metas in new_fork_city.\n",
      "new_fork_city - finished processing - output shape - torch.Size([3582, 3, 256, 256])\n",
      "2301 JPGs, 2263 metas in lindsaymaitland.\n",
      "lindsaymaitland - finished processing - output shape - torch.Size([2300, 3, 256, 256])\n",
      "1051 JPGs, 1007 metas in gatherandfeast.\n",
      "gatherandfeast - finished processing - output shape - torch.Size([1050, 3, 256, 256])\n",
      "2054 JPGs, 1962 metas in rushyama.\n",
      "rushyama - finished processing - output shape - torch.Size([2053, 3, 256, 256])\n",
      "7928 JPGs, 7910 metas in foodbabyny.\n",
      "foodbabyny - finished processing - output shape - torch.Size([7927, 3, 256, 256])\n",
      "2460 JPGs, 2324 metas in joythebaker.\n",
      "joythebaker - finished processing - output shape - torch.Size([2459, 3, 256, 256])\n",
      "4 JPGs, 4 metas in sarahlynnfitness.\n",
      "sarahlynnfitness - finished processing - output shape - torch.Size([3, 3, 256, 256])\n",
      "427 JPGs, 415 metas in discover.vi.\n",
      "discover.vi - finished processing - output shape - torch.Size([426, 3, 256, 256])\n",
      "573 JPGs, 564 metas in organicandhappy.\n",
      "organicandhappy - finished processing - output shape - torch.Size([572, 3, 256, 256])\n",
      "2367 JPGs, 2367 metas in halfbakedharvest.\n",
      "halfbakedharvest - finished processing - output shape - torch.Size([2366, 3, 256, 256])\n",
      "655 JPGs, 620 metas in nourish_atelier.\n",
      "nourish_atelier - finished processing - output shape - torch.Size([654, 3, 256, 256])\n",
      "3544 JPGs, 3468 metas in annabarnettcooks.\n",
      "annabarnettcooks - finished processing - output shape - torch.Size([3543, 3, 256, 256])\n",
      "1040 JPGs, 1040 metas in taraobrady.\n",
      "taraobrady - finished processing - output shape - torch.Size([1039, 3, 256, 256])\n",
      "319 JPGs, 313 metas in judy.\n",
      "judy - finished processing - output shape - torch.Size([318, 3, 256, 256])\n",
      "771 JPGs, 755 metas in reneredzepinoma.\n",
      "reneredzepinoma - finished processing - output shape - torch.Size([770, 3, 256, 256])\n",
      "1098 JPGs, 1086 metas in choosingchia.\n",
      "choosingchia - finished processing - output shape - torch.Size([1097, 3, 256, 256])\n",
      "1319 JPGs, 1293 metas in plantbasedjane.\n",
      "plantbasedjane - finished processing - output shape - torch.Size([1318, 3, 256, 256])\n",
      "3139 JPGs, 3138 metas in sherreenl.\n",
      "sherreenl - finished processing - output shape - torch.Size([3138, 3, 256, 256])\n",
      "63 JPGs, 63 metas in dandoherty_.\n",
      "dandoherty_ - finished processing - output shape - torch.Size([62, 3, 256, 256])\n",
      "1699 JPGs, 1654 metas in skyemcalpine.\n",
      "skyemcalpine - finished processing - output shape - torch.Size([1698, 3, 256, 256])\n",
      "7472 JPGs, 7342 metas in foodandwine.\n",
      "foodandwine - finished processing - output shape - torch.Size([7471, 3, 256, 256])\n",
      "8764 JPGs, 8616 metas in food52.\n",
      "food52 - finished processing - output shape - torch.Size([8763, 3, 256, 256])\n",
      "9958 JPGs, 9217 metas in clerkenwellboyec1.\n",
      "clerkenwellboyec1 - finished processing - output shape - torch.Size([9957, 3, 256, 256])\n",
      "1926 JPGs, 1906 metas in yossyarefi.\n",
      "yossyarefi - finished processing - output shape - torch.Size([1925, 3, 256, 256])\n",
      "3282 JPGs, 3118 metas in atasteofkoko.\n",
      "atasteofkoko - finished processing - output shape - torch.Size([3281, 3, 256, 256])\n",
      "2228 JPGs, 2208 metas in thefauxmartha.\n",
      "thefauxmartha - finished processing - output shape - torch.Size([2227, 3, 256, 256])\n",
      "3470 JPGs, 3421 metas in eatingnyc.\n",
      "eatingnyc - finished processing - output shape - torch.Size([3469, 3, 256, 256])\n",
      "464 JPGs, 463 metas in cookrepublic.\n",
      "cookrepublic - finished processing - output shape - torch.Size([463, 3, 256, 256])\n",
      "697 JPGs, 656 metas in thehungrywarrior.\n",
      "thehungrywarrior - finished processing - output shape - torch.Size([696, 3, 256, 256])\n",
      "729 JPGs, 708 metas in effifoods.\n",
      "effifoods - finished processing - output shape - torch.Size([728, 3, 256, 256])\n",
      "870 JPGs, 813 metas in shutthekaleup.\n",
      "shutthekaleup - finished processing - output shape - torch.Size([869, 3, 256, 256])\n",
      "514 JPGs, 413 metas in rachaelsgoodeats.\n",
      "rachaelsgoodeats - finished processing - output shape - torch.Size([513, 3, 256, 256])\n",
      "2039 JPGs, 1898 metas in mollyyeh.\n",
      "mollyyeh - finished processing - output shape - torch.Size([2037, 3, 256, 256])\n",
      "980 JPGs, 975 metas in closetcooking.\n",
      "closetcooking - finished processing - output shape - torch.Size([979, 3, 256, 256])\n",
      "4845 JPGs, 4643 metas in brunchboys.\n",
      "brunchboys - finished processing - output shape - torch.Size([4837, 3, 256, 256])\n",
      "202 JPGs, 163 metas in saladforpresident.\n",
      "saladforpresident - finished processing - output shape - torch.Size([201, 3, 256, 256])\n",
      "2178 JPGs, 2176 metas in helenedujardin.\n",
      "helenedujardin - finished processing - output shape - torch.Size([2177, 3, 256, 256])\n",
      "401 JPGs, 363 metas in justinbsamson.\n",
      "justinbsamson - finished processing - output shape - torch.Size([400, 3, 256, 256])\n",
      "1093 JPGs, 1077 metas in thelittleplantation.\n",
      "thelittleplantation - finished processing - output shape - torch.Size([1092, 3, 256, 256])\n",
      "584 JPGs, 564 metas in spoonforkbacon.\n",
      "spoonforkbacon - finished processing - output shape - torch.Size([583, 3, 256, 256])\n",
      "783 JPGs, 463 metas in deliciousaus.\n",
      "deliciousaus - finished processing - output shape - torch.Size([782, 3, 256, 256])\n",
      "219 JPGs, 214 metas in cottagefarm.\n",
      "cottagefarm - finished processing - output shape - torch.Size([218, 3, 256, 256])\n",
      "1006 JPGs, 957 metas in sproutedkitchen.\n",
      "sproutedkitchen - finished processing - output shape - torch.Size([1005, 3, 256, 256])\n",
      "1185 JPGs, 1181 metas in idafrosk.\n",
      "idafrosk - finished processing - output shape - torch.Size([1184, 3, 256, 256])\n",
      "380 JPGs, 380 metas in thebarefoothousewife.\n",
      "thebarefoothousewife - finished processing - output shape - torch.Size([379, 3, 256, 256])\n",
      "2889 JPGs, 2889 metas in dennistheprescott.\n",
      "dennistheprescott - finished processing - output shape - torch.Size([2888, 3, 256, 256])\n",
      "1547 JPGs, 1416 metas in alphafoodie.\n",
      "alphafoodie - finished processing - output shape - torch.Size([1546, 3, 256, 256])\n",
      "4248 JPGs, 4179 metas in deliciouslyella.\n",
      "deliciouslyella - finished processing - output shape - torch.Size([4247, 3, 256, 256])\n",
      "429 JPGs, 405 metas in muttipomodoroau.\n",
      "muttipomodoroau - finished processing - output shape - torch.Size([428, 3, 256, 256])\n",
      "370 JPGs, 363 metas in themodernproper.\n",
      "themodernproper - finished processing - output shape - torch.Size([369, 3, 256, 256])\n",
      "127 JPGs, 113 metas in anisa.\n",
      "anisa - finished processing - output shape - torch.Size([126, 3, 256, 256])\n",
      "877 JPGs, 865 metas in dad_beets.\n",
      "dad_beets - finished processing - output shape - torch.Size([876, 3, 256, 256])\n",
      "2068 JPGs, 2040 metas in drizzleanddip.\n",
      "drizzleanddip - finished processing - output shape - torch.Size([2067, 3, 256, 256])\n",
      "125 JPGs, 125 metas in chefjacqueslamerde.\n",
      "chefjacqueslamerde - finished processing - output shape - torch.Size([124, 3, 256, 256])\n",
      "871 JPGs, 829 metas in donna.hay.\n",
      "donna.hay - finished processing - output shape - torch.Size([870, 3, 256, 256])\n",
      "266 JPGs, 264 metas in careynotcarrie.\n",
      "careynotcarrie - finished processing - output shape - torch.Size([265, 3, 256, 256])\n",
      "642 JPGs, 613 metas in linda_lomelino.\n",
      "linda_lomelino - finished processing - output shape - torch.Size([641, 3, 256, 256])\n",
      "137 JPGs, 113 metas in grandbabycakes.\n",
      "grandbabycakes - finished processing - output shape - torch.Size([136, 3, 256, 256])\n",
      "3392 JPGs, 3360 metas in gimmesomeoven.\n",
      "gimmesomeoven - finished processing - output shape - torch.Size([3391, 3, 256, 256])\n",
      "1384 JPGs, 1359 metas in katie_clova.\n",
      "katie_clova - finished processing - output shape - torch.Size([1383, 3, 256, 256])\n",
      "2763 JPGs, 2602 metas in whatsgabycookin.\n",
      "whatsgabycookin - finished processing - output shape - torch.Size([2762, 3, 256, 256])\n",
      "2538 JPGs, 2532 metas in davehagerman.\n",
      "davehagerman - finished processing - output shape - torch.Size([2537, 3, 256, 256])\n",
      "924 JPGs, 923 metas in thecutlerychronicles.\n",
      "thecutlerychronicles - finished processing - output shape - torch.Size([923, 3, 256, 256])\n",
      "689 JPGs, 676 metas in raw_manda.\n",
      "raw_manda - finished processing - output shape - torch.Size([688, 3, 256, 256])\n",
      "549 JPGs, 467 metas in felicityspector.\n",
      "felicityspector - finished processing - output shape - torch.Size([548, 3, 256, 256])\n",
      "1 JPGs, 1 metas in happyfoody.\n",
      "happyfoody - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "2288 JPGs, 2205 metas in chekmarkeats.\n",
      "chekmarkeats - finished processing - output shape - torch.Size([2287, 3, 256, 256])\n",
      "3154 JPGs, 3129 metas in madeleine_shaw_.\n",
      "madeleine_shaw_ - finished processing - output shape - torch.Size([3153, 3, 256, 256])\n",
      "1021 JPGs, 963 metas in maggie_beer.\n",
      "maggie_beer - finished processing - output shape - torch.Size([1020, 3, 256, 256])\n",
      "1783 JPGs, 1747 metas in ameliafreer.\n",
      "ameliafreer - finished processing - output shape - torch.Size([1782, 3, 256, 256])\n",
      "2159 JPGs, 2100 metas in judy.kim.\n",
      "judy.kim - finished processing - output shape - torch.Size([2158, 3, 256, 256])\n",
      "1 JPGs, 1 metas in thejudylab.\n",
      "thejudylab - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "1436 JPGs, 1358 metas in smallmangalleypgh.\n",
      "smallmangalleypgh - finished processing - output shape - torch.Size([1435, 3, 256, 256])\n",
      "692 JPGs, 692 metas in lisa.eats.\n",
      "lisa.eats - finished processing - output shape - torch.Size([691, 3, 256, 256])\n",
      "1645 JPGs, 1476 metas in abrowntable.\n",
      "abrowntable - finished processing - output shape - torch.Size([1644, 3, 256, 256])\n",
      "574 JPGs, 529 metas in nelrestaurant.\n",
      "nelrestaurant - finished processing - output shape - torch.Size([573, 3, 256, 256])\n",
      "1780 JPGs, 1761 metas in ashrod.\n",
      "ashrod - finished processing - output shape - torch.Size([1779, 3, 256, 256])\n",
      "3072 JPGs, 3050 metas in theboywhobakes.\n",
      "theboywhobakes - finished processing - output shape - torch.Size([3071, 3, 256, 256])\n",
      "2699 JPGs, 2588 metas in emikodavies.\n",
      "emikodavies - finished processing - output shape - torch.Size([2698, 3, 256, 256])\n",
      "536 JPGs, 536 metas in therawboy.\n",
      "therawboy - finished processing - output shape - torch.Size([535, 3, 256, 256])\n",
      "322 JPGs, 313 metas in thedefineddish.\n",
      "thedefineddish - finished processing - output shape - torch.Size([321, 3, 256, 256])\n",
      "1417 JPGs, 1405 metas in _foodstories_.\n",
      "_foodstories_ - finished processing - output shape - torch.Size([1416, 3, 256, 256])\n",
      "3287 JPGs, 3192 metas in thenaughtyfork.\n",
      "thenaughtyfork - finished processing - output shape - torch.Size([3286, 3, 256, 256])\n",
      "182 JPGs, 163 metas in rainbowplantlife.\n",
      "rainbowplantlife - finished processing - output shape - torch.Size([181, 3, 256, 256])\n",
      "164 JPGs, 163 metas in stevehansenimages.\n",
      "stevehansenimages - finished processing - output shape - torch.Size([163, 3, 256, 256])\n",
      "1 JPGs, 1 metas in kaleandcaramel.\n",
      "kaleandcaramel - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "9034 JPGs, 7374 metas in ks_ate_here.\n",
      "ks_ate_here - finished processing - output shape - torch.Size([9021, 3, 256, 256])\n",
      "4818 JPGs, 4597 metas in inspiralized.\n",
      "inspiralized - finished processing - output shape - torch.Size([4817, 3, 256, 256])\n",
      "256 JPGs, 214 metas in leesamantha.\n",
      "leesamantha - finished processing - output shape - torch.Size([255, 3, 256, 256])\n",
      "144 JPGs, 115 metas in rawvana.\n",
      "rawvana - finished processing - output shape - torch.Size([143, 3, 256, 256])\n",
      "5493 JPGs, 5438 metas in gennarocontaldo.\n",
      "gennarocontaldo - finished processing - output shape - torch.Size([5492, 3, 256, 256])\n",
      "17 JPGs, 13 metas in shelbysorrel.\n",
      "shelbysorrel - finished processing - output shape - torch.Size([16, 3, 256, 256])\n",
      "4155 JPGs, 4000 metas in caitsplate.\n",
      "caitsplate - finished processing - output shape - torch.Size([4154, 3, 256, 256])\n",
      "7541 JPGs, 6915 metas in jamieoliver.\n",
      "jamieoliver - finished processing - output shape - torch.Size([7540, 3, 256, 256])\n",
      "990 JPGs, 716 metas in sarah_c_owens.\n",
      "sarah_c_owens - finished processing - output shape - torch.Size([989, 3, 256, 256])\n",
      "527 JPGs, 527 metas in vegan.dy.\n",
      "vegan.dy - finished processing - output shape - torch.Size([526, 3, 256, 256])\n",
      "978 JPGs, 969 metas in foooodieee.\n",
      "foooodieee - finished processing - output shape - torch.Size([977, 3, 256, 256])\n",
      "0 JPGs, 0 metas in data.\n",
      "data - finished processing - output shape - torch.Size([0, 3, 256, 256])\n",
      "807 JPGs, 803 metas in jo_rodgers.\n",
      "jo_rodgers - finished processing - output shape - torch.Size([806, 3, 256, 256])\n",
      "1763 JPGs, 1722 metas in artfuldesperado.\n",
      "artfuldesperado - finished processing - output shape - torch.Size([1762, 3, 256, 256])\n",
      "179 JPGs, 163 metas in jakesfoodbible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jakesfoodbible - finished processing - output shape - torch.Size([178, 3, 256, 256])\n",
      "1350 JPGs, 1316 metas in cashewkitchen.\n",
      "cashewkitchen - finished processing - output shape - torch.Size([1349, 3, 256, 256])\n",
      "2813 JPGs, 2582 metas in evansungnyc.\n",
      "evansungnyc - finished processing - output shape - torch.Size([2807, 3, 256, 256])\n",
      "837 JPGs, 703 metas in cocktailsbykurtis.\n",
      "cocktailsbykurtis - finished processing - output shape - torch.Size([833, 3, 256, 256])\n",
      "427 JPGs, 416 metas in thewholefooddiary.\n",
      "thewholefooddiary - finished processing - output shape - torch.Size([426, 3, 256, 256])\n",
      "1367 JPGs, 1352 metas in em.peachy.\n",
      "em.peachy - finished processing - output shape - torch.Size([1366, 3, 256, 256])\n",
      "963 JPGs, 918 metas in maurizio.\n",
      "maurizio - finished processing - output shape - torch.Size([962, 3, 256, 256])\n",
      "3514 JPGs, 3464 metas in jakecohen.\n",
      "jakecohen - finished processing - output shape - torch.Size([3503, 3, 256, 256])\n",
      "341 JPGs, 305 metas in peacelovedonutsnaplesfl.\n",
      "peacelovedonutsnaplesfl - finished processing - output shape - torch.Size([340, 3, 256, 256])\n",
      "2300 JPGs, 2300 metas in issycroker.\n",
      "issycroker - finished processing - output shape - torch.Size([2299, 3, 256, 256])\n",
      "1860 JPGs, 1844 metas in whatforbreakfast.\n",
      "whatforbreakfast - finished processing - output shape - torch.Size([1859, 3, 256, 256])\n",
      "832 JPGs, 763 metas in twolovesstudio.\n",
      "twolovesstudio - finished processing - output shape - torch.Size([831, 3, 256, 256])\n",
      "472 JPGs, 463 metas in ohhoneybakes.\n",
      "ohhoneybakes - finished processing - output shape - torch.Size([471, 3, 256, 256])\n",
      "268 JPGs, 263 metas in georgeats.\n",
      "georgeats - finished processing - output shape - torch.Size([267, 3, 256, 256])\n",
      "19 JPGs, 13 metas in sammyandbellaofficial.\n",
      "sammyandbellaofficial - finished processing - output shape - torch.Size([18, 3, 256, 256])\n",
      "787 JPGs, 782 metas in thaliaho.\n",
      "thaliaho - finished processing - output shape - torch.Size([786, 3, 256, 256])\n",
      "2078 JPGs, 2061 metas in succulentbite.\n",
      "succulentbite - finished processing - output shape - torch.Size([2077, 3, 256, 256])\n",
      "2228 JPGs, 2218 metas in julskitchen.\n",
      "julskitchen - finished processing - output shape - torch.Size([2227, 3, 256, 256])\n",
      "465 JPGs, 426 metas in gatton_michelle.\n",
      "gatton_michelle - finished processing - output shape - torch.Size([464, 3, 256, 256])\n",
      "1587 JPGs, 1584 metas in signebay.\n",
      "signebay - finished processing - output shape - torch.Size([1586, 3, 256, 256])\n",
      "2604 JPGs, 2554 metas in andreabemis.\n",
      "andreabemis - finished processing - output shape - torch.Size([2603, 3, 256, 256])\n",
      "1469 JPGs, 1373 metas in madzpayne.\n",
      "madzpayne - finished processing - output shape - torch.Size([1468, 3, 256, 256])\n",
      "5456 JPGs, 5322 metas in howsweeteats.\n",
      "howsweeteats - finished processing - output shape - torch.Size([5455, 3, 256, 256])\n",
      "3236 JPGs, 3073 metas in ottolenghi.\n",
      "ottolenghi - finished processing - output shape - torch.Size([3235, 3, 256, 256])\n",
      "834 JPGs, 816 metas in blondieandrye.\n",
      "blondieandrye - finished processing - output shape - torch.Size([833, 3, 256, 256])\n",
      "752 JPGs, 663 metas in laurentoyota.\n",
      "laurentoyota - finished processing - output shape - torch.Size([751, 3, 256, 256])\n",
      "2500 JPGs, 2459 metas in buzzfeedfood.\n",
      "buzzfeedfood - 2000 images processed\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b58aa22dd7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mydim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mjson_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTC_(\\\\d+)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'UTC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json.xz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import os,sys,cv2,gc,json,lzma,re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "xdim = 256\n",
    "ydim = 256\n",
    "metadata = []\n",
    "\n",
    "dfolder = '/home/jok120/ml/proj/data/old_data/tags/tmp/'\n",
    "\n",
    "for folder in os.listdir(dfolder):\n",
    "    n = 0\n",
    "    n_int = 1\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    path_folder = os.path.join(dfolder,folder)\n",
    "\n",
    "    if not (os.path.isdir(path_folder)):# and folder.startswith('#')):\n",
    "        continue\n",
    "        \n",
    "    jpgCounter = len(glob.glob1(path_folder,\"*.jpg\"))\n",
    "    metadataCounter = len(glob.glob1(path_folder,\"*.json.xz\"))\n",
    "    print(jpgCounter, \"JPGs,\", metadataCounter, \"metas in {}.\".format(folder))\n",
    "    images = torch.empty(jpgCounter,3,xdim,ydim)\n",
    "    stored_indices = []\n",
    "\n",
    "    for f in os.listdir(path_folder):\n",
    "        fname, fext = os.path.splitext(f)\n",
    "\n",
    "        if fext != '.jpg':\n",
    "            continue\n",
    "\n",
    "        #img = imread(os.path.join(dfolder,folder,f)) / 255\n",
    "        #img = torch.from_numpy(cv2.resize(img, dsize=(ydim,xdim)).transpose(2,0,1)[None,:]).type(torch.FloatTensor)\n",
    "        img = Image.open(os.path.join(dfolder,folder,f))\n",
    "        img = img.resize((ydim,xdim))\n",
    "        img = np.asarray(img) / 255\n",
    "        img = torch.from_numpy(img.transpose(2,0,1)).type(torch.FloatTensor)\n",
    "\n",
    "        json_filename = os.path.join(dfolder,folder,re.sub('UTC_(\\\\d+)','UTC',fname)+'.json.xz')\n",
    "\n",
    "        try:\n",
    "            json_str = lzma.open(json_filename).read()\n",
    "        except:\n",
    "            continue\n",
    "        json_data = json.loads(json_str)\n",
    "        metadata.append(json_data)\n",
    "        \n",
    "        images[n] = img\n",
    "        stored_indices.append(n)\n",
    "        n += 1\n",
    "        \n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print('{} - {} images processed'.format(folder,n),end='\\r')\n",
    "    images = images[stored_indices]\n",
    "    print('{} - finished processing - output shape - {}'.format(folder,images.shape))\n",
    "    assert images.shape[0] == len(metadata), \"non-matching number of images/metas\"\n",
    "    torch.save(images,os.path.join(dfolder,'{}.tch'.format(folder)))\n",
    "    pkl.dump(metadata,open(os.path.join(dfolder,'{}_metadata.pkl'.format(folder)),'wb'))\n",
    "    \n",
    "    images = torch.empty(0,3,xdim,ydim)\n",
    "    metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open(os.path.join('/home/jok120/ml/proj/data/old_data/tags/tmp/abrowntable_metadata.pkl'),'rb'))\n",
    "d = torch.load('/home/jok120/ml/proj/data/old_data/tags/tmp/abrowntable.tch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 1645)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m), len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls /home/jok120/ml/proj/data/old_data/tags/tmp/*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "18 18\n",
      "1870 1870\n",
      "1882 1882\n",
      "2045 2045\n",
      "2445 2445\n",
      "3045 3045\n",
      "3851 3851\n",
      "5549 5549\n",
      "6569 6569\n",
      "7514 7514\n",
      "9814 9814\n",
      "10054 10054\n",
      "13557 13557\n",
      "14426 14426\n",
      "14550 14550\n",
      "14550 14550\n",
      "16136 16136\n",
      "19024 19024\n",
      "19024 19024\n",
      "19450 19450\n",
      "22212 22212\n",
      "770 770\n",
      "1461 1461\n",
      "1830 1830\n",
      "2294 2294\n",
      "6406 6406\n",
      "6727 6727\n",
      "8702 8702\n",
      "9625 9625\n",
      "10353 10353\n",
      "13160 13160\n",
      "13477 13477\n",
      "13477 13477\n",
      "15530 15530\n",
      "18067 18067\n",
      "25994 25994\n",
      "8763 8763\n",
      "11129 11129\n",
      "13588 13588\n",
      "14627 14627\n",
      "23648 23648\n",
      "0 0\n",
      "426 426\n",
      "693 693\n",
      "819 819\n",
      "1388 1388\n",
      "2796 2796\n",
      "6950 6950\n",
      "7093 7093\n",
      "8476 8476\n",
      "8741 8741\n",
      "9314 9314\n",
      "11391 11391\n",
      "12032 12032\n",
      "12048 12048\n",
      "12924 12924\n",
      "13363 13363\n",
      "15882 15882\n",
      "19273 19273\n",
      "20106 20106\n",
      "1468 1468\n",
      "11425 11425\n",
      "12841 12841\n",
      "14485 14485\n",
      "19977 19977\n",
      "20295 20295\n",
      "3 3\n",
      "1561 1561\n",
      "7016 7016\n",
      "10251 10251\n",
      "11435 11435\n",
      "11753 11753\n",
      "14356 14356\n",
      "17637 17637\n",
      "18108 18108\n",
      "19158 19158\n",
      "19294 19294\n",
      "21361 21361\n",
      "3469 3469\n",
      "5251 5251\n",
      "5916 5916\n",
      "6171 6171\n",
      "7002 7002\n",
      "8007 8007\n",
      "8590 8590\n",
      "9103 9103\n",
      "10452 10452\n",
      "11770 11770\n",
      "15056 15056\n",
      "15744 15744\n",
      "17506 17506\n",
      "19613 19613\n",
      "19613 19613\n",
      "21173 21173\n",
      "463 463\n",
      "12201 12201\n",
      "13019 13019\n",
      "17266 17266\n",
      "17653 17653\n",
      "17715 17715\n",
      "18263 18263\n",
      "18464 18464\n",
      "21162 21162\n",
      "751 751\n",
      "8291 8291\n",
      "9388 9388\n",
      "10365 10365\n",
      "10900 10900\n",
      "13951 13951\n",
      "14784 14784\n",
      "14965 14965\n",
      "15268 15268\n",
      "17445 17445\n",
      "18424 18424\n",
      "20349 20349\n",
      "782 782\n",
      "960 960\n",
      "2506 2506\n",
      "3078 3078\n",
      "6231 6231\n",
      "7017 7017\n",
      "8876 8876\n",
      "9255 9255\n",
      "10217 10217\n",
      "12504 12504\n",
      "12932 12932\n",
      "21176 21176\n",
      "2299 2299\n",
      "5370 5370\n",
      "7597 7597\n",
      "15068 15068\n",
      "17295 17295\n",
      "18387 18387\n",
      "19041 19041\n",
      "19381 19381\n",
      "19485 19485\n",
      "19486 19486\n",
      "21265 21265\n",
      "656 656\n",
      "3794 3794\n",
      "4783 4783\n",
      "5001 5001\n",
      "7038 7038\n",
      "8404 8404\n",
      "13241 13241\n",
      "14676 14676\n",
      "17809 17809\n",
      "17821 17821\n",
      "19176 19176\n",
      "19715 19715\n",
      "23258 23258\n",
      "3582 3582\n",
      "4108 4108\n",
      "8886 8886\n",
      "11044 11044\n",
      "15861 15861\n",
      "16731 16731\n",
      "18230 18230\n",
      "18926 18926\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "dfolder = '/home/jok120/ml/proj/data/old_data/tags/tmp/'\n",
    "wfolder = '/home/jok120/ml/training_data/small'\n",
    "\n",
    "\n",
    "X_train = torch.empty((0,3,xdim,ydim))\n",
    "X_meta = []\n",
    "\n",
    "N = 1\n",
    "\n",
    "for f in os.listdir(dfolder):\n",
    "    fname, fext = os.path.splitext(f)\n",
    "    \n",
    "    if fext != '.tch':\n",
    "        continue\n",
    "        \n",
    "    X_train = torch.cat([X_train,torch.load(os.path.join(dfolder,f))])\n",
    "    X_meta += pickle.load(open(os.path.join(dfolder,fname+'_metadata.pkl'),'rb'))\n",
    "    print(len(X_train),len(X_meta))\n",
    "    if len(X_train) > 20000:\n",
    "        torch.save(X_train,os.path.join(wfolder,'train_data_{}.tch'.format(N)))\n",
    "        pickle.dump(X_meta,open(os.path.join(wfolder,'train_meta_{}.pkl'.format(N)),'wb'))\n",
    "        \n",
    "        N += 1\n",
    "        \n",
    "        X_train = torch.empty((0,3,xdim,ydim))\n",
    "        X_meta = []\n",
    "        \n",
    "torch.save(X_train,os.path.join(wfolder,'train_data_{}.tch'.format(N)))\n",
    "pickle.dump(X_meta,open(os.path.join(wfolder,'train_meta_{}.pkl'.format(N)),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.empty((0,3,64,64))\n",
    "torch.cat([torch.ones(0,3,64,64),torch.ones(8,3,64,64)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(images,os.path.join(dfolder,'data/#homecooking.tch'.format(folder)))\n",
    "pkl.dump(metadata,open(os.path.join(dfolder,'data/{}_metadata.pkl'.format(folder)),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_dlproj)",
   "language": "python",
   "name": "pytorch_dlproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
